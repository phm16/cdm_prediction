---
title: "Data Science using Apache Spark and RStudio sparklyr"
author: "Zach Sharp"
date: "November 6, 2016"
output:
  pdf_document: default
  html_notebook: default
linkcolor: blue
urlcolor: blue
---

*PERMISSION TO SHARE WITH OTHERS; NEW TOPIC:  SPARK*

Two common criticisms of using R for "Big Data" analysis are 1) the data analyzed in R is typically limited to what can fit into memory on a PC and 2) R does not implicitly support multi-core processing.  [Apache Spark](http://spark.apache.org/), a “fast and general engine for large-scale data processing,” can be used to alleviate some of these bottlenecks.  The Spark framework provides SQL manipulation, stream processing, and machine learning on a clustered compute environment and enables parallel processing.  It can be interfaced using multiple languages such as Java, Scala, Python, and R.

[RStudio](https://www.rstudio.com/) has recently released the [sparklyr](https://blog.rstudio.org/2016/09/27/sparklyr-r-interface-for-apache-spark/) package, which provides a front-end to access Spark and provides a higher-level set of functions to make it easier for analysts and data scientists who know R, but not necessarily low-level programming details, to leverage Spark.

Since Spark can run in multiple environments and access data from many different sources, it offers the potential to streamline the data science workflow.  Some of the potential value of using Spark includes:

* Running complex querying and data manipulation on very large datasets from within R
* Eliminating intermediate data processing toolsets to get data to fit into R
* Reducing machine learning model training time by leveraging parallel processing
* Reducing potential IT bottlenecks by not requiring specific schemas and ETL processes (data can be read directly to Spark)

Our project team plans to utilize the interface to the Spark Machine Learning library (MLlib) to see if we can reduce the training time on our models so that we can iterate faster.  Additionally, when we put our models into a production environment, we will want to include more data in our models to improve our model performance.  As more processing data is collected, the files may not fit into memory on local machines and may require a cluster solution, which is another reason Spark is an attractive option to improve our efficiency.

Below is an example of training and evaluating a random forest model on the PHM team's transformed dataset from the [PHM 2016 Data Challenge](https://www.phmsociety.org/events/conference/phm/16/data-challenge):

## Connect to a local Spark cluster

```{r, message = FALSE, warning = FALSE}
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local")
```

## Copy data to Spark cluster

```{r}
train_df_spark <- spark_read_csv(
  sc, 
  name = "train_df_spark",
  path = "../data/train_df.csv",
  overwrite = TRUE,
  memory = FALSE
)
```

## Partition the data into training and test sets

```{r}
partitions <- sdf_partition(train_df_spark, training = 0.7, test = 0.3)
```

## Model performance function (MSE)

```{r}
estimate_mse <- function(df, model) {
  
  sdf_predict(model, df) %>%
    mutate(resid = AVG_REMOVAL_RATE - prediction) %>%
    summarize(MSE = mean(resid ^ 2)) %>%
    collect()
  
}
```

## Train a random forest

```{r}
spark_rf <- partitions$training %>%
  ml_random_forest(
    response = "AVG_REMOVAL_RATE",
    features = c(
      "A_1",
      "A_4",
      "B_4",
      "CENTER_AIR_BAG_PRESSURE_max_P123", 
      "CENTER_AIR_BAG_PRESSURE_sum_P123", 
      "USAGE_OF_DRESSER_max",
      "SLURRY_FLOW_LINE_A_mean_P123",
      "SLURRY_FLOW_LINE_B_mean_P123",
      "SLURRY_FLOW_LINE_C_mean_P123",
      "STAGE_ROTATION_mean_P123",
      "HEAD_ROTATION_mean_P123"
      ),
    type = "regression",
    num.trees = 500
  )
```

## Evaluate model performance

```{r}
sapply(partitions, estimate_mse, model = spark_rf)   
```
